{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1370f200-4985-4c08-bb9c-e26f3ecc58f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Simple Binary Classification\n",
    "\n",
    "The Sonar Dataset involves the prediction of whether or not an object is a mine or a rock given the strength of sonar returns at different angles. It is a binary (2-class) classification problem. The number of observations for each class is not balanced. There are **208** observations with **60** input variables and 1 output variable. The variable names are as follows:\n",
    "\n",
    "Sonar returns at different angles\n",
    "...\n",
    "Class (M for mine and R for rock)\n",
    "\n",
    "The baseline performance of predicting the most prevalent class is a classification accuracy of approximately 53%. Top results achieve a classification accuracy of approximately 88%.\n",
    "\n",
    "1. Split the data into training and test (consider class imbalance).\n",
    "2. Choose a performance function suitable for the problem.\n",
    "3. Adjust the following algorithms to the data using the default parameters: Linear Regression, Logistic Regression, SVM, KNN, Decision trees.\n",
    "4. Select the algorithm with the highest performance measure.\n",
    "5. Report the performance on the test data.\n",
    "\n",
    "Database\n",
    "\n",
    "[https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.csv)\n",
    "\n",
    "More information\n",
    "\n",
    "[https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))\n",
    "\n",
    "## References\n",
    "- [top-julia-machine-learning-libraries](https://www.analyticsvidhya.com/blog/2021/05/top-julia-machine-learning-libraries/)\n",
    "- [Julia language in machine learning: Algorithms, applications, and open\n",
    "issues](https://www.sciencedirect.com/science/article/pii/S157401372030071X)\n",
    "- [MLJ.jl](https://alan-turing-institute.github.io/MLJ.jl/dev/)\n",
    "- [ScikitLearn.jl](https://cstjean.github.io/ScikitLearn.jl/dev/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304efac3-e406-46a5-accb-865d5dccd5c0",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12b152a6-31b5-4f31-bdf5-e5a8a672100b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.webio.node+json": {
       "children": [],
       "instanceArgs": {
        "namespace": "html",
        "tag": "div"
       },
       "nodeType": "DOM",
       "props": {},
       "type": "node"
      },
      "text/html": [
       "<div style=\"padding: 1em; background-color: #f8d6da; border: 1px solid #f5c6cb; font-weight: bold;\">\n",
       "<p>The WebIO Jupyter extension was not detected. See the\n",
       "<a href=\"https://juliagizmos.github.io/WebIO.jl/latest/providers/ijulia/\" target=\"_blank\">\n",
       "    WebIO Jupyter integration documentation\n",
       "</a>\n",
       "for more information.\n",
       "</div>\n"
      ],
      "text/plain": [
       "WebIO._IJuliaInit()"
      ]
     },
     "metadata": {
      "@webio": {
       "kernelId": "47048d35-255a-478f-a93e-9210abbaabdd"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m new project at `C:\\Users\\sebas\\.julia\\environments\\my_env`\n",
      "┌ Warning: Error requiring `WebSockets` from `WebIO`\n",
      "│   exception = (LoadError(\"C:\\\\Users\\\\sebas\\\\.julia\\\\packages\\\\WebIO\\\\VF9p5\\\\src\\\\providers\\\\generic_http.jl\", 15, ErrorException(\"Unable to find WebIO JavaScript bundle for generic HTTP provider; try rebuilding WebIO (via `Pkg.build(\\\"WebIO\\\")`).\")), Union{Ptr{Nothing}, Base.InterpreterIP}[Ptr{Nothing} @0x00000000105bb8e2, Ptr{Nothing} @0x00000000027cb64c, Ptr{Nothing} @0x00000000027cb071, Ptr{Nothing} @0x00000000027cbc40, Ptr{Nothing} @0x00000000027cc942, Base.InterpreterIP in top-level CodeInfo for WebIO at statement 4, Ptr{Nothing} @0x00000000027ea14f, Ptr{Nothing} @0x00000000027eac41, Ptr{Nothing} @0x00000000027ebcaf, Ptr{Nothing} @0x0000000010a8dc81, Ptr{Nothing} @0x000000005ed9253f, Ptr{Nothing} @0x00000000027cb64c, Ptr{Nothing} @0x00000000027cb071, Ptr{Nothing} @0x00000000027cbc40, Ptr{Nothing} @0x00000000027cc942, Base.InterpreterIP in top-level CodeInfo for WebIO at statement 10, Ptr{Nothing} @0x00000000027ea14f, Ptr{Nothing} @0x00000000027ebcaf, Ptr{Nothing} @0x000000005eda7f4f, Ptr{Nothing} @0x000000005eda7f73, Ptr{Nothing} @0x00000000027cb64c, Ptr{Nothing} @0x00000000027cb071, Ptr{Nothing} @0x00000000027cc07a, Ptr{Nothing} @0x00000000027cc12d, Ptr{Nothing} @0x00000000027cc5f0, Base.InterpreterIP in MethodInstance for Requires.err(::Any, ::Module, ::String, ::String, ::Any) at statement 8, Ptr{Nothing} @0x000000005eda7ea6, Ptr{Nothing} @0x000000005eda7ec3, Ptr{Nothing} @0x00000000027cb64c, Ptr{Nothing} @0x00000000027cb071, Ptr{Nothing} @0x00000000027cc07a, Ptr{Nothing} @0x00000000027cc12d, Ptr{Nothing} @0x00000000027cc5f0, Base.InterpreterIP in MethodInstance for Requires.withpath(::Any, ::String) at statement 10, Ptr{Nothing} @0x000000005eda7dbf, Ptr{Nothing} @0x000000005eda7e33, Ptr{Nothing} @0x00000000027bd1b6, Ptr{Nothing} @0x000000005ed78aa1, Ptr{Nothing} @0x00000000027cb64c, Ptr{Nothing} @0x00000000027cb071, Ptr{Nothing} @0x00000000027cbea6, Ptr{Nothing} @0x00000000027cc5f0, Base.InterpreterIP in MethodInstance for Requires.loadpkg(::Base.PkgId) at statement 6, Ptr{Nothing} @0x00000000027bd1b6, Ptr{Nothing} @0x000000005ed6efde, Ptr{Nothing} @0x000000005ed6dc7b, Ptr{Nothing} @0x000000005ed6eee8, Ptr{Nothing} @0x000000005ed6dc7b, Ptr{Nothing} @0x000000005ed7088f, Ptr{Nothing} @0x000000005ed72943, Ptr{Nothing} @0x000000005ed915c3, Ptr{Nothing} @0x00000000027e92f6, Ptr{Nothing} @0x00000000027eb135, Ptr{Nothing} @0x00000000027cbe22, Ptr{Nothing} @0x00000000027cc942, Base.InterpreterIP in top-level CodeInfo for Main at statement 0, Ptr{Nothing} @0x00000000027ea14f, Ptr{Nothing} @0x00000000027ebcaf, Ptr{Nothing} @0x000000005ed907a0, Ptr{Nothing} @0x000000005ed90912, Ptr{Nothing} @0x000000005ed90a23, Ptr{Nothing} @0x000000005ed90a43, Ptr{Nothing} @0x00000000027cb64c, Ptr{Nothing} @0x00000000027cb071, Ptr{Nothing} @0x00000000027cbc40, Ptr{Nothing} @0x00000000027cc942, Base.InterpreterIP in top-level CodeInfo for Main at statement 0, Ptr{Nothing} @0x00000000027ea14f, Ptr{Nothing} @0x00000000027eac41, Ptr{Nothing} @0x00000000027ebcaf, Ptr{Nothing} @0x000000005ed21bf5, Ptr{Nothing} @0x000000005ed21fef, Ptr{Nothing} @0x000000005ed1fceb, Ptr{Nothing} @0x00000000027bd1b6, Ptr{Nothing} @0x000000005ed18279, Ptr{Nothing} @0x000000005ed18786, Ptr{Nothing} @0x000000005ed187a3, Ptr{Nothing} @0x00000000027d0c9f])\n",
      "└ @ Requires C:\\Users\\sebas\\.julia\\packages\\Requires\\Z8rfN\\src\\require.jl:51\n",
      "┌ Warning: Kaledio is not available on this system. Julia will be unable to produce any plots.\n",
      "└ @ PlotlyBase C:\\Users\\sebas\\.julia\\packages\\PlotlyBase\\NxSlF\\src\\kaleido.jl:58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Plots.PlotlyJSBackend()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV, DataFrames, Plots, Random, MLDataUtils, Printf, Pkg, NearestNeighborModels, DecisionTree, Statistics, MLJ #,ScikitLearn, MLJScikitLearnInterface \n",
    "Pkg.activate(\"my_env\", shared=true)\n",
    "plotlyjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b4c797d-97b8-4e97-872f-c037c7eee291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>10 rows × 7 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title=\"Symbol\">Symbol</th><th title=\"Union{Nothing, Float64}\">Union…</th><th title=\"Any\">Any</th><th title=\"Union{Nothing, Float64}\">Union…</th><th title=\"Any\">Any</th><th title=\"Int64\">Int64</th><th title=\"DataType\">DataType</th></tr></thead><tbody><tr><th>1</th><td>Type</td><td></td><td>M</td><td></td><td>R</td><td>0</td><td>String1</td></tr><tr><th>2</th><td>a1</td><td>0.0291639</td><td>0.0015</td><td>0.0228</td><td>0.1371</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>a2</td><td>0.0384365</td><td>0.0006</td><td>0.0308</td><td>0.2339</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>a3</td><td>0.0438322</td><td>0.0015</td><td>0.0343</td><td>0.3059</td><td>0</td><td>Float64</td></tr><tr><th>5</th><td>a4</td><td>0.0538923</td><td>0.0058</td><td>0.04405</td><td>0.4264</td><td>0</td><td>Float64</td></tr><tr><th>6</th><td>a5</td><td>0.0752024</td><td>0.0067</td><td>0.0625</td><td>0.401</td><td>0</td><td>Float64</td></tr><tr><th>7</th><td>a6</td><td>0.10457</td><td>0.0102</td><td>0.09215</td><td>0.3823</td><td>0</td><td>Float64</td></tr><tr><th>8</th><td>a7</td><td>0.121747</td><td>0.0033</td><td>0.10695</td><td>0.3729</td><td>0</td><td>Float64</td></tr><tr><th>9</th><td>a8</td><td>0.134799</td><td>0.0055</td><td>0.1121</td><td>0.459</td><td>0</td><td>Float64</td></tr><tr><th>10</th><td>a9</td><td>0.178003</td><td>0.0075</td><td>0.15225</td><td>0.6828</td><td>0</td><td>Float64</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & Type &  & M &  & R & 0 & String1 \\\\\n",
       "\t2 & a1 & 0.0291639 & 0.0015 & 0.0228 & 0.1371 & 0 & Float64 \\\\\n",
       "\t3 & a2 & 0.0384365 & 0.0006 & 0.0308 & 0.2339 & 0 & Float64 \\\\\n",
       "\t4 & a3 & 0.0438322 & 0.0015 & 0.0343 & 0.3059 & 0 & Float64 \\\\\n",
       "\t5 & a4 & 0.0538923 & 0.0058 & 0.04405 & 0.4264 & 0 & Float64 \\\\\n",
       "\t6 & a5 & 0.0752024 & 0.0067 & 0.0625 & 0.401 & 0 & Float64 \\\\\n",
       "\t7 & a6 & 0.10457 & 0.0102 & 0.09215 & 0.3823 & 0 & Float64 \\\\\n",
       "\t8 & a7 & 0.121747 & 0.0033 & 0.10695 & 0.3729 & 0 & Float64 \\\\\n",
       "\t9 & a8 & 0.134799 & 0.0055 & 0.1121 & 0.459 & 0 & Float64 \\\\\n",
       "\t10 & a9 & 0.178003 & 0.0075 & 0.15225 & 0.6828 & 0 & Float64 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m10×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable \u001b[0m\u001b[1m mean      \u001b[0m\u001b[1m min    \u001b[0m\u001b[1m median  \u001b[0m\u001b[1m max    \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol   \u001b[0m\u001b[90m Union…    \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────────────────────\n",
       "   1 │ Type     \u001b[90m           \u001b[0m M      \u001b[90m         \u001b[0m R              0  String1\n",
       "   2 │ a1        0.0291639  0.0015  0.0228   0.1371         0  Float64\n",
       "   3 │ a2        0.0384365  0.0006  0.0308   0.2339         0  Float64\n",
       "   4 │ a3        0.0438322  0.0015  0.0343   0.3059         0  Float64\n",
       "   5 │ a4        0.0538923  0.0058  0.04405  0.4264         0  Float64\n",
       "   6 │ a5        0.0752024  0.0067  0.0625   0.401          0  Float64\n",
       "   7 │ a6        0.10457    0.0102  0.09215  0.3823         0  Float64\n",
       "   8 │ a7        0.121747   0.0033  0.10695  0.3729         0  Float64\n",
       "   9 │ a8        0.134799   0.0055  0.1121   0.459          0  Float64\n",
       "  10 │ a9        0.178003   0.0075  0.15225  0.6828         0  Float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSonar = CSV.read(\"Datasets/sonar.csv\", DataFrame)\n",
    "first(describe(dataSonar),10) #|> pretty\n",
    "#show(dataSonar, allrows=true, allcols=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b222a-73de-4bc0-9d8f-74de8768b5e8",
   "metadata": {},
   "source": [
    "Since the Type column (label) is the type string let's change it to int using the following convention\n",
    "$$\n",
    "\\begin{matrix} \\text{Rock (R)} & \\rightarrow & 0 \\\\ \\text{Mine (M)} & \\rightarrow & 1 \\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8ee7359-e88a-466b-a542-ffc607b73be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>5 rows × 7 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title=\"Symbol\">Symbol</th><th title=\"Union{Nothing, Float64}\">Union…</th><th title=\"Any\">Any</th><th title=\"Union{Nothing, Float64}\">Union…</th><th title=\"Any\">Any</th><th title=\"Int64\">Int64</th><th title=\"DataType\">DataType</th></tr></thead><tbody><tr><th>1</th><td>Type</td><td></td><td>M</td><td></td><td>R</td><td>0</td><td>String1</td></tr><tr><th>2</th><td>a1</td><td>0.0291639</td><td>0.0015</td><td>0.0228</td><td>0.1371</td><td>0</td><td>Float64</td></tr><tr><th>3</th><td>a2</td><td>0.0384365</td><td>0.0006</td><td>0.0308</td><td>0.2339</td><td>0</td><td>Float64</td></tr><tr><th>4</th><td>a3</td><td>0.0438322</td><td>0.0015</td><td>0.0343</td><td>0.3059</td><td>0</td><td>Float64</td></tr><tr><th>5</th><td>a4</td><td>0.0538923</td><td>0.0058</td><td>0.04405</td><td>0.4264</td><td>0</td><td>Float64</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& variable & mean & min & median & max & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Int64 & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & Type &  & M &  & R & 0 & String1 \\\\\n",
       "\t2 & a1 & 0.0291639 & 0.0015 & 0.0228 & 0.1371 & 0 & Float64 \\\\\n",
       "\t3 & a2 & 0.0384365 & 0.0006 & 0.0308 & 0.2339 & 0 & Float64 \\\\\n",
       "\t4 & a3 & 0.0438322 & 0.0015 & 0.0343 & 0.3059 & 0 & Float64 \\\\\n",
       "\t5 & a4 & 0.0538923 & 0.0058 & 0.04405 & 0.4264 & 0 & Float64 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m variable \u001b[0m\u001b[1m mean      \u001b[0m\u001b[1m min    \u001b[0m\u001b[1m median  \u001b[0m\u001b[1m max    \u001b[0m\u001b[1m nmissing \u001b[0m\u001b[1m eltype   \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Symbol   \u001b[0m\u001b[90m Union…    \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Union…  \u001b[0m\u001b[90m Any    \u001b[0m\u001b[90m Int64    \u001b[0m\u001b[90m DataType \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────────────────────\n",
       "   1 │ Type     \u001b[90m           \u001b[0m M      \u001b[90m         \u001b[0m R              0  String1\n",
       "   2 │ a1        0.0291639  0.0015  0.0228   0.1371         0  Float64\n",
       "   3 │ a2        0.0384365  0.0006  0.0308   0.2339         0  Float64\n",
       "   4 │ a3        0.0438322  0.0015  0.0343   0.3059         0  Float64\n",
       "   5 │ a4        0.0538923  0.0058  0.04405  0.4264         0  Float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function string_to_float(str)\n",
    "    if str==\"R\"\n",
    "        return 0.0\n",
    "    else\n",
    "        return 1.0\n",
    "    end\n",
    "end\n",
    "dataSonar.TypeInt = map(string -> string_to_float(string), dataSonar.Type)\n",
    "describe(dataSonar)[1:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f1ce21-ff1d-4e95-b60a-7e77decd9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "dataSonar_plot  = DataFrame()\n",
    "dataSonar_plot.Ind = Array((1:size(dataSonar,1)));\n",
    "dataSonar_plot.Avg = mean.(eachrow(dataSonar[:,2:end]))\n",
    "dataSonar_plot.Type = dataSonar.Type;\n",
    "=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d66f072-d615-42cc-bc5e-5327b2d7d847",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "KeyError: key \"R\" not found",
     "output_type": "error",
     "traceback": [
      "KeyError: key \"R\" not found",
      "",
      "Stacktrace:",
      " [1] getindex(h::Dict{Int64, String}, key::String1)",
      "   @ Base .\\dict.jl:481",
      " [2] top-level scope",
      "   @ .\\In[5]:8",
      " [3] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "species_color_map = Dict(1 => \"blue\",     0 => \"green\")\n",
    "species_name      = Dict(1 => \"Mine (M)\", 0 => \"Rock (R)\")\n",
    "\n",
    "l = @layout [a{0.5w} b]\n",
    "p = plot(st = [:scatter, :scatter], layout = l, size=(1000,400), plot_title=\"Example\")\n",
    "\n",
    "for subdf in groupby(dataSonar, :Type)\n",
    "    plot!(p, subdf.a1, subdf.a10, seriestype = :scatter, label=species_name[subdf.Type[1]], xlabel=\"a1\", ylabel=\"a10\",  title=\"2D Example\") #scatter(x, y, f, st = [:surface, :contourf], layout = l)\n",
    "    plot!(p[2], subdf.a1, subdf.a20, subdf.a30, seriestype = :scatter, label=\"\", markersize = 2, xlabel=\"a1\", ylabel=\"a20\", zlabel=\"a30\",  title=\"3D Example\")\n",
    "end\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46737814-4f5a-48cd-97f0-2940cd257987",
   "metadata": {},
   "source": [
    "where $x$, $y$, and $z$ in the right plot are $a_1$, $a_{20}$, and $a_{30}$, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba946362-bec7-4b85-ac82-5b4fd3b939d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Data Percentage with R: 47 %, Data Percentage with M: 53 %\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rdata, Mdata = groupby(dataSonar, :Type)\n",
    "NRpart, NMpart = 100*size(Rdata,1)/(size(Rdata,1) + size(Mdata,1)), 100*size(Mdata,1)/(size(Rdata,1) + size(Mdata,1))\n",
    "\n",
    "#println(\"Porcentaje datos con R: $NRpart % \\n Porcentaje datos con M: $NMpart %\")\n",
    "@sprintf(\"Data Percentage with R: %.0f %s, Data Percentage with M: %.0f %s\", NRpart, \"%\", NMpart, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea86b001-42c2-4eaa-9680-cab24e164236",
   "metadata": {},
   "source": [
    "### Train and Test Datasets\n",
    "Let's splite the datasets in a $85:15$ relation, train-test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "023657a5-7b9d-435c-a422-f25bf9459e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffleobs(dataSonar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "493f3729-76d2-47ff-8665-5f090bf996d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\u001b[1m176×62 SubDataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Type    \u001b[0m\u001b[1m a1      \u001b[0m\u001b[1m a2      \u001b[0m\u001b[1m a3      \u001b[0m\u001b[1m a4      \u001b[0m\u001b[1m a5      \u001b[0m\u001b[1m a6      \u001b[0m\u001b[1m a7      \u001b[0m\u001b[1m\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m String1 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ M         0.0712   0.0901   0.1276   0.1497   0.1284   0.1165   0.1285  ⋯\n",
       "   2 │ M         0.0211   0.0128   0.0015   0.045    0.0711   0.1563   0.1518\n",
       "   3 │ M         0.0116   0.0179   0.0449   0.1096   0.1913   0.0924   0.0761\n",
       "   4 │ M         0.0197   0.0394   0.0384   0.0076   0.0251   0.0629   0.0747\n",
       "   5 │ R         0.0453   0.0523   0.0843   0.0689   0.1183   0.2583   0.2156  ⋯\n",
       "   6 │ M         0.079    0.0707   0.0352   0.166    0.133    0.0226   0.0771\n",
       "   7 │ R         0.0201   0.0116   0.0123   0.0245   0.0547   0.0208   0.0891\n",
       "   8 │ R         0.0442   0.0477   0.0049   0.0581   0.0278   0.0678   0.1664\n",
       "   9 │ R         0.0131   0.0068   0.0308   0.0311   0.0085   0.0767   0.0771  ⋯\n",
       "  10 │ M         0.0158   0.0239   0.015    0.0494   0.0988   0.1425   0.1463\n",
       "  11 │ M         0.0137   0.0297   0.0116   0.0082   0.0241   0.0253   0.0279\n",
       "  ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮     ⋱\n",
       " 167 │ R         0.0365   0.1632   0.1636   0.1421   0.113    0.1306   0.2112\n",
       " 168 │ M         0.0094   0.0333   0.0306   0.0376   0.1296   0.1795   0.1909  ⋯\n",
       " 169 │ M         0.0368   0.0279   0.0103   0.0566   0.0759   0.0679   0.097\n",
       " 170 │ R         0.0195   0.0142   0.0181   0.0406   0.0391   0.0249   0.0892\n",
       " 171 │ R         0.0293   0.0644   0.039    0.0173   0.0476   0.0816   0.0993\n",
       " 172 │ M         0.0249   0.0119   0.0277   0.076    0.1218   0.1538   0.1192  ⋯\n",
       " 173 │ R         0.0123   0.0309   0.0169   0.0313   0.0358   0.0102   0.0182\n",
       " 174 │ R         0.0229   0.0369   0.004    0.0375   0.0455   0.1452   0.2211\n",
       " 175 │ M         0.0131   0.0201   0.0045   0.0217   0.023    0.0481   0.0742\n",
       " 176 │ M         0.0392   0.0108   0.0267   0.0257   0.041    0.0491   0.1053  ⋯\n",
       "\u001b[36m                                                 54 columns and 155 rows omitted\u001b[0m, \u001b[1m32×62 SubDataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Type    \u001b[0m\u001b[1m a1      \u001b[0m\u001b[1m a2      \u001b[0m\u001b[1m a3      \u001b[0m\u001b[1m a4      \u001b[0m\u001b[1m a5      \u001b[0m\u001b[1m a6      \u001b[0m\u001b[1m a7      \u001b[0m\u001b[1m\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m String1 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ M         0.0163   0.0198   0.0202   0.0386   0.0752   0.1444   0.1487  ⋯\n",
       "   2 │ M         0.0307   0.0523   0.0653   0.0521   0.0611   0.0577   0.0665\n",
       "   3 │ M         0.0587   0.121    0.1268   0.1498   0.1436   0.0561   0.0832\n",
       "   4 │ R         0.0065   0.0122   0.0068   0.0108   0.0217   0.0284   0.0527\n",
       "   5 │ M         0.0235   0.022    0.0167   0.0516   0.0746   0.1121   0.1258  ⋯\n",
       "   6 │ R         0.0308   0.0339   0.0202   0.0889   0.157    0.175    0.092\n",
       "   7 │ R         0.0519   0.0548   0.0842   0.0319   0.1158   0.0922   0.1027\n",
       "   8 │ M         0.0721   0.1574   0.1112   0.1085   0.0666   0.18     0.1108\n",
       "   9 │ R         0.01     0.0275   0.019    0.0371   0.0416   0.0201   0.0314  ⋯\n",
       "  10 │ R         0.0293   0.0378   0.0257   0.0062   0.013    0.0612   0.0895\n",
       "  11 │ M         0.0346   0.0509   0.0079   0.0243   0.0432   0.0735   0.0938\n",
       "  ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮     ⋱\n",
       "  23 │ R         0.0274   0.0242   0.0621   0.056    0.1129   0.0973   0.1823\n",
       "  24 │ M         0.0329   0.0216   0.0386   0.0627   0.1158   0.1482   0.2054  ⋯\n",
       "  25 │ M         0.0107   0.0453   0.0289   0.0713   0.1075   0.1019   0.1606\n",
       "  26 │ M         0.0388   0.0324   0.0688   0.0898   0.1267   0.1515   0.2134\n",
       "  27 │ M         0.043    0.0902   0.0833   0.0813   0.0165   0.0277   0.0569\n",
       "  28 │ M         0.0323   0.0101   0.0298   0.0564   0.076    0.0958   0.099   ⋯\n",
       "  29 │ M         0.0454   0.0472   0.0697   0.1021   0.1397   0.1493   0.1487\n",
       "  30 │ R         0.0188   0.037    0.0953   0.0824   0.0249   0.0488   0.1424\n",
       "  31 │ R         0.0195   0.0213   0.0058   0.019    0.0319   0.0571   0.1004\n",
       "  32 │ R         0.0265   0.044    0.0137   0.0084   0.0305   0.0438   0.0341  ⋯\n",
       "\u001b[36m                                                  54 columns and 11 rows omitted\u001b[0m)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R1, R2 = splitobs(shuffleobs(Rdata), at = 0.85)\n",
    "M1, M2 = splitobs(shuffleobs(Mdata), at = 0.85)\n",
    "\n",
    "train, test = shuffleobs([R1; M1]), shuffleobs([R2; M2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "dfbe8eff-0554-44bb-8c2c-df7e095f0b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\u001b[1m32×60 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m a1      \u001b[0m\u001b[1m a2      \u001b[0m\u001b[1m a3      \u001b[0m\u001b[1m a4      \u001b[0m\u001b[1m a5      \u001b[0m\u001b[1m a6      \u001b[0m\u001b[1m a7      \u001b[0m\u001b[1m a8      \u001b[0m\u001b[1m\u001b[0m ⋯\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m\u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │  0.0163   0.0198   0.0202   0.0386   0.0752   0.1444   0.1487   0.1484  ⋯\n",
       "   2 │  0.0307   0.0523   0.0653   0.0521   0.0611   0.0577   0.0665   0.0664\n",
       "   3 │  0.0587   0.121    0.1268   0.1498   0.1436   0.0561   0.0832   0.0672\n",
       "   4 │  0.0065   0.0122   0.0068   0.0108   0.0217   0.0284   0.0527   0.0575\n",
       "   5 │  0.0235   0.022    0.0167   0.0516   0.0746   0.1121   0.1258   0.1717  ⋯\n",
       "   6 │  0.0308   0.0339   0.0202   0.0889   0.157    0.175    0.092    0.1353\n",
       "   7 │  0.0519   0.0548   0.0842   0.0319   0.1158   0.0922   0.1027   0.0613\n",
       "   8 │  0.0721   0.1574   0.1112   0.1085   0.0666   0.18     0.1108   0.2794\n",
       "   9 │  0.01     0.0275   0.019    0.0371   0.0416   0.0201   0.0314   0.0651  ⋯\n",
       "  10 │  0.0293   0.0378   0.0257   0.0062   0.013    0.0612   0.0895   0.1107\n",
       "  11 │  0.0346   0.0509   0.0079   0.0243   0.0432   0.0735   0.0938   0.1134\n",
       "  ⋮  │    ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮        ⋮     ⋱\n",
       "  23 │  0.0274   0.0242   0.0621   0.056    0.1129   0.0973   0.1823   0.1745\n",
       "  24 │  0.0329   0.0216   0.0386   0.0627   0.1158   0.1482   0.2054   0.1605  ⋯\n",
       "  25 │  0.0107   0.0453   0.0289   0.0713   0.1075   0.1019   0.1606   0.2119\n",
       "  26 │  0.0388   0.0324   0.0688   0.0898   0.1267   0.1515   0.2134   0.2613\n",
       "  27 │  0.043    0.0902   0.0833   0.0813   0.0165   0.0277   0.0569   0.2057\n",
       "  28 │  0.0323   0.0101   0.0298   0.0564   0.076    0.0958   0.099    0.1018  ⋯\n",
       "  29 │  0.0454   0.0472   0.0697   0.1021   0.1397   0.1493   0.1487   0.0771\n",
       "  30 │  0.0188   0.037    0.0953   0.0824   0.0249   0.0488   0.1424   0.1972\n",
       "  31 │  0.0195   0.0213   0.0058   0.019    0.0319   0.0571   0.1004   0.0668\n",
       "  32 │  0.0265   0.044    0.0137   0.0084   0.0305   0.0438   0.0341   0.078   ⋯\n",
       "\u001b[36m                                                  52 columns and 11 rows omitted\u001b[0m, [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0  …  0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_Label, Y_test_Label  = CategoricalArray(train[:,1]), CategoricalArray(test[:,1])\n",
    "\n",
    "X_train, Y_train = train[:,2:end-1], train[:,end]\n",
    "X_test, Y_test = test[:,2:end-1], test[:,end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6b7d29c-39a2-4003-8939-fe3b44b4c814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size Train Datase: (176, 62), Size Test Datase: (32, 62)\n"
     ]
    }
   ],
   "source": [
    "Strain, Stest = size(train), size(test)\n",
    "println(\"Size Train Datase: $Strain, Size Test Datase: $Stest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f680b-5c0e-4395-af38-26f732283a42",
   "metadata": {},
   "source": [
    "Show almost all models availables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "597a269a-4e50-45e0-bb85-cb50824754f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x, y = @load_boston;\n",
    "#ms = models(matching(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c2b0e5-7bb0-43cd-ac03-0707764fd09f",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "The MLJ.jl library will be used to call, train, and evaluate the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e98ba4b5-b7bc-4a31-bb79-3aa07d7d6e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "LinearRegressor\n",
       "\\end{verbatim}\n",
       "A model type for constructing a ordinary least-squares regressor (OLS), based on \\href{https://github.com/cstjean/ScikitLearn.jl}{ScikitLearn.jl}, and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "\\begin{verbatim}\n",
       "LinearRegressor = @load LinearRegressor pkg=ScikitLearn\n",
       "\\end{verbatim}\n",
       "Do \\texttt{model = LinearRegressor()} to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in \\texttt{LinearRegressor(fit\\_intercept=...)}.\n",
       "\n",
       "\\section{Hyper-parameters}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{fit\\_intercept = true}\n",
       "\n",
       "\n",
       "\\item \\texttt{normalize = false}\n",
       "\n",
       "\n",
       "\\item \\texttt{copy\\_X = true}\n",
       "\n",
       "\n",
       "\\item \\texttt{n\\_jobs = nothing}\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "LinearRegressor\n",
       "```\n",
       "\n",
       "A model type for constructing a ordinary least-squares regressor (OLS), based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "```\n",
       "LinearRegressor = @load LinearRegressor pkg=ScikitLearn\n",
       "```\n",
       "\n",
       "Do `model = LinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LinearRegressor(fit_intercept=...)`.\n",
       "\n",
       "# Hyper-parameters\n",
       "\n",
       "  * `fit_intercept = true`\n",
       "  * `normalize = false`\n",
       "  * `copy_X = true`\n",
       "  * `n_jobs = nothing`\n"
      ],
      "text/plain": [
       "\u001b[36m  LinearRegressor\u001b[39m\n",
       "\n",
       "  A model type for constructing a ordinary least-squares regressor (OLS),\n",
       "  based on ScikitLearn.jl (https://github.com/cstjean/ScikitLearn.jl), and\n",
       "  implementing the MLJ model interface.\n",
       "\n",
       "  From MLJ, the type can be imported using\n",
       "\n",
       "\u001b[36m  LinearRegressor = @load LinearRegressor pkg=ScikitLearn\u001b[39m\n",
       "\n",
       "  Do \u001b[36mmodel = LinearRegressor()\u001b[39m to construct an instance with default\n",
       "  hyper-parameters. Provide keyword arguments to override hyper-parameter\n",
       "  defaults, as in \u001b[36mLinearRegressor(fit_intercept=...)\u001b[39m.\n",
       "\n",
       "\u001b[1m  Hyper-parameters\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mfit_intercept = true\u001b[39m\n",
       "\n",
       "    •  \u001b[36mnormalize = false\u001b[39m\n",
       "\n",
       "    •  \u001b[36mcopy_X = true\u001b[39m\n",
       "\n",
       "    •  \u001b[36mn_jobs = nothing\u001b[39m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc(\"LinearRegressor\", pkg=\"ScikitLearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2b3ff59f-2a6a-48d4-a991-cdfaf88ac249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{LinearRegressor,…}.\n",
      "└ @ MLJBase C:\\Users\\sebas\\.julia\\packages\\MLJBase\\rMXo2\\src\\machines.jl:423\n",
      "┌ Info: Solver: MLJLinearModels.Analytical\n",
      "│   iterative: Bool false\n",
      "│   max_inner: Int64 200\n",
      "└ @ MLJLinearModels C:\\Users\\sebas\\.julia\\packages\\MLJLinearModels\\2qDvV\\src\\mlj\\interface.jl:39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, measurement, operation, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_pairs\n",
       "Extract:\n",
       "┌────────────────────────┬─────────────┬───────────┬────────────────────────────\n",
       "│\u001b[22m measure                \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold                 \u001b[0m ⋯\n",
       "├────────────────────────┼─────────────┼───────────┼────────────────────────────\n",
       "│ LPLoss(p = 1)          │ 0.431       │ predict   │ [0.458, 0.467, 0.402, 0.4 ⋯\n",
       "│ RootMeanSquaredError() │ 0.533       │ predict   │ [0.513, 0.533, 0.498, 0.5 ⋯\n",
       "└────────────────────────┴─────────────┴───────────┴────────────────────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJ\n",
    "using MLJ:fit!, predict, evaluate\n",
    "\n",
    "#Xtrain = Float64.(Xtrain)\n",
    "LinearRegressor = @load LinearRegressor pkg=MLJLinearModels verbosity=0\n",
    "\n",
    "mach = fit!(machine(LinearRegressor(), X_train, Y_train))\n",
    "#fitted_params(mach)\n",
    "#Y_predict_Linear = round.(predict(mach, X_test))\n",
    "\n",
    "evaluate(LinearRegressor(), X_train, Y_train,  measure=[l1, rms], verbosity=0)  #, resampling = [(X_train, Y_train), (X_test, Y_test)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc2bfe85-b6aa-4743-8cd1-9082f24353b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "LogisticClassifier\n",
       "\\end{verbatim}\n",
       "A model type for constructing a logistic regression classifier, based on \\href{https://github.com/cstjean/ScikitLearn.jl}{ScikitLearn.jl}, and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "\\begin{verbatim}\n",
       "LogisticClassifier = @load LogisticClassifier pkg=ScikitLearn\n",
       "\\end{verbatim}\n",
       "Do \\texttt{model = LogisticClassifier()} to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in \\texttt{LogisticClassifier(penalty=...)}.\n",
       "\n",
       "\\section{Hyper-parameters}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{penalty = l2}\n",
       "\n",
       "\n",
       "\\item \\texttt{dual = false}\n",
       "\n",
       "\n",
       "\\item \\texttt{tol = 0.0001}\n",
       "\n",
       "\n",
       "\\item \\texttt{C = 1.0}\n",
       "\n",
       "\n",
       "\\item \\texttt{fit\\_intercept = true}\n",
       "\n",
       "\n",
       "\\item \\texttt{intercept\\_scaling = 1.0}\n",
       "\n",
       "\n",
       "\\item \\texttt{class\\_weight = nothing}\n",
       "\n",
       "\n",
       "\\item \\texttt{random\\_state = nothing}\n",
       "\n",
       "\n",
       "\\item \\texttt{solver = lbfgs}\n",
       "\n",
       "\n",
       "\\item \\texttt{max\\_iter = 100}\n",
       "\n",
       "\n",
       "\\item \\texttt{multi\\_class = auto}\n",
       "\n",
       "\n",
       "\\item \\texttt{verbose = 0}\n",
       "\n",
       "\n",
       "\\item \\texttt{warm\\_start = false}\n",
       "\n",
       "\n",
       "\\item \\texttt{n\\_jobs = nothing}\n",
       "\n",
       "\n",
       "\\item \\texttt{l1\\_ratio = nothing}\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "LogisticClassifier\n",
       "```\n",
       "\n",
       "A model type for constructing a logistic regression classifier, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "```\n",
       "LogisticClassifier = @load LogisticClassifier pkg=ScikitLearn\n",
       "```\n",
       "\n",
       "Do `model = LogisticClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `LogisticClassifier(penalty=...)`.\n",
       "\n",
       "# Hyper-parameters\n",
       "\n",
       "  * `penalty = l2`\n",
       "  * `dual = false`\n",
       "  * `tol = 0.0001`\n",
       "  * `C = 1.0`\n",
       "  * `fit_intercept = true`\n",
       "  * `intercept_scaling = 1.0`\n",
       "  * `class_weight = nothing`\n",
       "  * `random_state = nothing`\n",
       "  * `solver = lbfgs`\n",
       "  * `max_iter = 100`\n",
       "  * `multi_class = auto`\n",
       "  * `verbose = 0`\n",
       "  * `warm_start = false`\n",
       "  * `n_jobs = nothing`\n",
       "  * `l1_ratio = nothing`\n"
      ],
      "text/plain": [
       "\u001b[36m  LogisticClassifier\u001b[39m\n",
       "\n",
       "  A model type for constructing a logistic regression classifier, based on\n",
       "  ScikitLearn.jl (https://github.com/cstjean/ScikitLearn.jl), and implementing\n",
       "  the MLJ model interface.\n",
       "\n",
       "  From MLJ, the type can be imported using\n",
       "\n",
       "\u001b[36m  LogisticClassifier = @load LogisticClassifier pkg=ScikitLearn\u001b[39m\n",
       "\n",
       "  Do \u001b[36mmodel = LogisticClassifier()\u001b[39m to construct an instance with default\n",
       "  hyper-parameters. Provide keyword arguments to override hyper-parameter\n",
       "  defaults, as in \u001b[36mLogisticClassifier(penalty=...)\u001b[39m.\n",
       "\n",
       "\u001b[1m  Hyper-parameters\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mpenalty = l2\u001b[39m\n",
       "\n",
       "    •  \u001b[36mdual = false\u001b[39m\n",
       "\n",
       "    •  \u001b[36mtol = 0.0001\u001b[39m\n",
       "\n",
       "    •  \u001b[36mC = 1.0\u001b[39m\n",
       "\n",
       "    •  \u001b[36mfit_intercept = true\u001b[39m\n",
       "\n",
       "    •  \u001b[36mintercept_scaling = 1.0\u001b[39m\n",
       "\n",
       "    •  \u001b[36mclass_weight = nothing\u001b[39m\n",
       "\n",
       "    •  \u001b[36mrandom_state = nothing\u001b[39m\n",
       "\n",
       "    •  \u001b[36msolver = lbfgs\u001b[39m\n",
       "\n",
       "    •  \u001b[36mmax_iter = 100\u001b[39m\n",
       "\n",
       "    •  \u001b[36mmulti_class = auto\u001b[39m\n",
       "\n",
       "    •  \u001b[36mverbose = 0\u001b[39m\n",
       "\n",
       "    •  \u001b[36mwarm_start = false\u001b[39m\n",
       "\n",
       "    •  \u001b[36mn_jobs = nothing\u001b[39m\n",
       "\n",
       "    •  \u001b[36ml1_ratio = nothing\u001b[39m"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc(\"LogisticClassifier\", pkg=\"ScikitLearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0e4047eb-e8aa-47b4-9554-4be0ceff684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{LogisticClassifier,…}.\n",
      "└ @ MLJBase C:\\Users\\sebas\\.julia\\packages\\MLJBase\\rMXo2\\src\\machines.jl:423\n",
      "┌ Info: Solver: MLJLinearModels.LBFGS()\n",
      "└ @ MLJLinearModels C:\\Users\\sebas\\.julia\\packages\\MLJLinearModels\\2qDvV\\src\\mlj\\interface.jl:76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, measurement, operation, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_pairs\n",
       "Extract:\n",
       "┌────────────────────────────┬─────────────┬───────────┬────────────────────────\n",
       "│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold             \u001b[0m ⋯\n",
       "├────────────────────────────┼─────────────┼───────────┼────────────────────────\n",
       "│ LogLoss(tol = 2.22045e-16) │ 0.714       │ predict   │ [0.686, 0.7, 0.729, 0 ⋯\n",
       "└────────────────────────────┴─────────────┴───────────┴────────────────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logistic = @load LogisticClassifier pkg=MLJLinearModels verbosity=0\n",
    "\n",
    "modelLog = Logistic(penalty = \"l2\")\n",
    "\n",
    "#fit!(machine(Logistic(penalty = \"l2\"), X_train, Y_train_Label))\n",
    "mach_log = fit!(machine(modelLog, X_train, Y_train_Label))\n",
    "#fitted_params(mach_log)\n",
    "Y_predict_Log = predict(mach_log, X_test)\n",
    "\n",
    "evaluate(modelLog, X_test, Y_test_Label, verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5abc6e4-467a-4ead-a452-ba713ab6328d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "SVMRegressor\n",
       "\\end{verbatim}\n",
       "A model type for constructing a epsilon-support vector regressor, based on \\href{https://github.com/cstjean/ScikitLearn.jl}{ScikitLearn.jl}, and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "\\begin{verbatim}\n",
       "SVMRegressor = @load SVMRegressor pkg=ScikitLearn\n",
       "\\end{verbatim}\n",
       "Do \\texttt{model = SVMRegressor()} to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in \\texttt{SVMRegressor(kernel=...)}.\n",
       "\n",
       "\\section{Hyper-parameters}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{kernel = rbf}\n",
       "\n",
       "\n",
       "\\item \\texttt{degree = 3}\n",
       "\n",
       "\n",
       "\\item \\texttt{gamma = auto}\n",
       "\n",
       "\n",
       "\\item \\texttt{coef0 = 0.0}\n",
       "\n",
       "\n",
       "\\item \\texttt{tol = 0.001}\n",
       "\n",
       "\n",
       "\\item \\texttt{C = 1.0}\n",
       "\n",
       "\n",
       "\\item \\texttt{epsilon = 0.1}\n",
       "\n",
       "\n",
       "\\item \\texttt{shrinking = true}\n",
       "\n",
       "\n",
       "\\item \\texttt{cache\\_size = 200}\n",
       "\n",
       "\n",
       "\\item \\texttt{max\\_iter = -1}\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "SVMRegressor\n",
       "```\n",
       "\n",
       "A model type for constructing a epsilon-support vector regressor, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "```\n",
       "SVMRegressor = @load SVMRegressor pkg=ScikitLearn\n",
       "```\n",
       "\n",
       "Do `model = SVMRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `SVMRegressor(kernel=...)`.\n",
       "\n",
       "# Hyper-parameters\n",
       "\n",
       "  * `kernel = rbf`\n",
       "  * `degree = 3`\n",
       "  * `gamma = auto`\n",
       "  * `coef0 = 0.0`\n",
       "  * `tol = 0.001`\n",
       "  * `C = 1.0`\n",
       "  * `epsilon = 0.1`\n",
       "  * `shrinking = true`\n",
       "  * `cache_size = 200`\n",
       "  * `max_iter = -1`\n"
      ],
      "text/plain": [
       "\u001b[36m  SVMRegressor\u001b[39m\n",
       "\n",
       "  A model type for constructing a epsilon-support vector regressor, based on\n",
       "  ScikitLearn.jl (https://github.com/cstjean/ScikitLearn.jl), and implementing\n",
       "  the MLJ model interface.\n",
       "\n",
       "  From MLJ, the type can be imported using\n",
       "\n",
       "\u001b[36m  SVMRegressor = @load SVMRegressor pkg=ScikitLearn\u001b[39m\n",
       "\n",
       "  Do \u001b[36mmodel = SVMRegressor()\u001b[39m to construct an instance with default\n",
       "  hyper-parameters. Provide keyword arguments to override hyper-parameter\n",
       "  defaults, as in \u001b[36mSVMRegressor(kernel=...)\u001b[39m.\n",
       "\n",
       "\u001b[1m  Hyper-parameters\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mkernel = rbf\u001b[39m\n",
       "\n",
       "    •  \u001b[36mdegree = 3\u001b[39m\n",
       "\n",
       "    •  \u001b[36mgamma = auto\u001b[39m\n",
       "\n",
       "    •  \u001b[36mcoef0 = 0.0\u001b[39m\n",
       "\n",
       "    •  \u001b[36mtol = 0.001\u001b[39m\n",
       "\n",
       "    •  \u001b[36mC = 1.0\u001b[39m\n",
       "\n",
       "    •  \u001b[36mepsilon = 0.1\u001b[39m\n",
       "\n",
       "    •  \u001b[36mshrinking = true\u001b[39m\n",
       "\n",
       "    •  \u001b[36mcache_size = 200\u001b[39m\n",
       "\n",
       "    •  \u001b[36mmax_iter = -1\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc(\"SVMRegressor\", pkg=\"ScikitLearn\")\n",
    "#SVMLClassifier, SVMLRegressor, SVMNuClassifier, SVMNuRegressor, SVMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf40364b-d4af-4ac2-8884-5b2bd8814a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{SVMRegressor,…}.\n",
      "└ @ MLJBase C:\\Users\\sebas\\.julia\\packages\\MLJBase\\rMXo2\\src\\machines.jl:423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, measurement, operation, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_pairs\n",
       "Extract:\n",
       "┌───────────────────────────────────────────────────┬─────────────┬─────────────\n",
       "│\u001b[22m measure                                           \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m ⋯\n",
       "├───────────────────────────────────────────────────┼─────────────┼─────────────\n",
       "│ LPLoss(p = 1)                                     │ 0.381       │ predict    ⋯\n",
       "│ RootMeanSquaredError()                            │ 0.431       │ predict    ⋯\n",
       "│ RootMeanSquaredLogProportionalError(offset = 1.0) │ 0.31        │ predict    ⋯\n",
       "└───────────────────────────────────────────────────┴─────────────┴─────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVMR = @load SVMRegressor pkg=ScikitLearn verbosity=0\n",
    "\n",
    "modelSVMR = SVMR()\n",
    "mach_SVMR = fit!(machine(modelSVMR, X_train, Y_train))\n",
    "#fitted_params(mach_SVMR)\n",
    "Y_predict_SVMR = round.(predict(mach_SVMR, X_test))\n",
    "\n",
    "evaluate(modelSVMR, X_train, Y_train,  measure=[l1, rms, rmslp1], verbosity=0)  #, resampling = [(X_train, Y_train), (X_test, Y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "810d2adf-1df6-4340-9f8e-46ee881160fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "K-Nearest Neighbors classifier: predicts the class associated with a new point by taking a vote over the classes of the K-nearest points.\n",
       "\n"
      ],
      "text/markdown": [
       "K-Nearest Neighbors classifier: predicts the class associated with a new point by taking a vote over the classes of the K-nearest points.\n"
      ],
      "text/plain": [
       "  K-Nearest Neighbors classifier: predicts the class associated with a new\n",
       "  point by taking a vote over the classes of the K-nearest points."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc(\"KNNClassifier\", pkg=\"NearestNeighborModels\")\n",
    "# KNNClassifier, KNNRegressor, MultitargetKNNClassifier, MultitargetKNNRegressor\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1cb673d1-1b77-434e-ae31-dc6fff57c53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{KNNClassifier,…}.\n",
      "└ @ MLJBase C:\\Users\\sebas\\.julia\\packages\\MLJBase\\rMXo2\\src\\machines.jl:423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, measurement, operation, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_pairs\n",
       "Extract:\n",
       "┌────────────────────────────┬─────────────┬───────────┬────────────────────────\n",
       "│\u001b[22m measure                    \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m│\u001b[22m per_fold             \u001b[0m ⋯\n",
       "├────────────────────────────┼─────────────┼───────────┼────────────────────────\n",
       "│ LogLoss(tol = 2.22045e-16) │ 1.59        │ predict   │ [1.77, 1.58, 2.79, 1. ⋯\n",
       "└────────────────────────────┴─────────────┴───────────┴────────────────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using NearestNeighborModels,  DataFrames, CategoricalArrays\n",
    "\n",
    "KNN = @load KNNClassifier pkg=NearestNeighborModels verbosity=0\n",
    "\n",
    "model_KNN = KNN()#weights = Inverse())\n",
    "Y_train_Label = CategoricalArray(train[:,1])\n",
    "mach_KNN = fit!(machine(KNN(), X_train, Y_train_Label))\n",
    "#fitted_params(mach_KNN)\n",
    "Y_predict_KNN = predict(mach_KNN, X_test)\n",
    "\n",
    "evaluate(model_KNN, X_train, Y_train_Label, verbosity=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "450b6106-3988-40ad-a742-f35b9831a64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "KNNClassifier(;kwargs...)\n",
       "\\end{verbatim}\n",
       "K-Nearest Neighbors classifier: predicts the class associated with a new point by taking a vote over the classes of the K-nearest points.\n",
       "\n",
       "\\subsection{Keywords Parameters}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{K::Int=5} : number of neighbors\n",
       "\n",
       "\n",
       "\\item \\texttt{algorithm::Symbol = :kdtree} : one of \\texttt{(:kdtree, :brutetree, :balltree)}\n",
       "\n",
       "\n",
       "\\item \\texttt{metric::Metric = Euclidean()} : any \\texttt{Metric} from    \\href{https://github.com/JuliaStats/Distances.jl}{Distances.jl} for the    distance between points. For \\texttt{algorithm = :kdtree} only metrics which are of    type \\texttt{Union\\{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski\\}} are supported.\n",
       "\n",
       "\n",
       "\\item \\texttt{leafsize::Int = algorithm == 10} : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as \\texttt{0}    for \\texttt{algorithm = :brutetree}, since \\texttt{brutetree} isn't actually a tree.\n",
       "\n",
       "\n",
       "\\item \\texttt{reorder::Bool = true} : if \\texttt{true} then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to \\texttt{true}    can significantly improve performance of the specified \\texttt{algorithm}    (except \\texttt{:brutetree}). This option is ignored and always taken as \\texttt{false} for    \\texttt{algorithm = :brutetree}.\n",
       "\n",
       "\n",
       "\\item \\texttt{weights::KNNKernel=Uniform()} : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    \\texttt{list\\_kernels()}. User-defined weighting functions can be passed by wrapping the    function in a \\texttt{UDF} kernel. If sample weights \\texttt{w} are passed during machine    construction e.g \\texttt{machine(model, X, y, w)} then the weight assigned to each    neighbor is the product of the \\texttt{KNNKernel} generated weight and the corresponding    neighbor sample weight.\n",
       "\n",
       "\\end{itemize}\n",
       "See also the  \\href{https://github.com/KristofferC/NearestNeighbors.jl}{package documentation}. For more information about the kernels see the paper by Geler et.al  \\href{https://perun.pmf.uns.ac.rs/radovanovic/publications/2016-kais-knn-weighting.pdf}{Comparison of different weighting schemes for the kNN classifier on time-series data}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "KNNClassifier(;kwargs...)\n",
       "```\n",
       "\n",
       "K-Nearest Neighbors classifier: predicts the class associated with a new point by taking a vote over the classes of the K-nearest points.\n",
       "\n",
       "## Keywords Parameters\n",
       "\n",
       "  * `K::Int=5` : number of neighbors\n",
       "  * `algorithm::Symbol = :kdtree` : one of `(:kdtree, :brutetree, :balltree)`\n",
       "  * `metric::Metric = Euclidean()` : any `Metric` from    [Distances.jl](https://github.com/JuliaStats/Distances.jl) for the    distance between points. For `algorithm = :kdtree` only metrics which are of    type `Union{Distances.Chebyshev, Distances.Cityblock, Distances.Euclidean, Distances.Minkowski, Distances.WeightedCityblock, Distances.WeightedEuclidean, Distances.WeightedMinkowski}` are supported.\n",
       "  * `leafsize::Int = algorithm == 10` : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as `0`    for `algorithm = :brutetree`, since `brutetree` isn't actually a tree.\n",
       "  * `reorder::Bool = true` : if `true` then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to `true`    can significantly improve performance of the specified `algorithm`    (except `:brutetree`). This option is ignored and always taken as `false` for    `algorithm = :brutetree`.\n",
       "  * `weights::KNNKernel=Uniform()` : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    `list_kernels()`. User-defined weighting functions can be passed by wrapping the    function in a `UDF` kernel. If sample weights `w` are passed during machine    construction e.g `machine(model, X, y, w)` then the weight assigned to each    neighbor is the product of the `KNNKernel` generated weight and the corresponding    neighbor sample weight.\n",
       "\n",
       "See also the  [package documentation](https://github.com/KristofferC/NearestNeighbors.jl). For more information about the kernels see the paper by Geler et.al  [Comparison of different weighting schemes for the kNN classifier on time-series data](https://perun.pmf.uns.ac.rs/radovanovic/publications/2016-kais-knn-weighting.pdf).\n"
      ],
      "text/plain": [
       "\u001b[36m  KNNClassifier(;kwargs...)\u001b[39m\n",
       "\n",
       "  K-Nearest Neighbors classifier: predicts the class associated with a new\n",
       "  point by taking a vote over the classes of the K-nearest points.\n",
       "\n",
       "\u001b[1m  Keywords Parameters\u001b[22m\n",
       "\u001b[1m  =====================\u001b[22m\n",
       "\n",
       "    •  \u001b[36mK::Int=5\u001b[39m : number of neighbors\n",
       "\n",
       "    •  \u001b[36malgorithm::Symbol = :kdtree\u001b[39m : one of \u001b[36m(:kdtree, :brutetree,\n",
       "       :balltree)\u001b[39m\n",
       "\n",
       "    •  \u001b[36mmetric::Metric = Euclidean()\u001b[39m : any \u001b[36mMetric\u001b[39m from Distances.jl\n",
       "       (https://github.com/JuliaStats/Distances.jl) for the distance\n",
       "       between points. For \u001b[36malgorithm = :kdtree\u001b[39m only metrics which are of\n",
       "       type \u001b[36mUnion{Distances.Chebyshev, Distances.Cityblock,\n",
       "       Distances.Euclidean, Distances.Minkowski,\n",
       "       Distances.WeightedCityblock, Distances.WeightedEuclidean,\n",
       "       Distances.WeightedMinkowski}\u001b[39m are supported.\n",
       "\n",
       "    •  \u001b[36mleafsize::Int = algorithm == 10\u001b[39m : determines the number of points\n",
       "       at which to stop splitting the tree. This option is ignored and\n",
       "       always taken as \u001b[36m0\u001b[39m for \u001b[36malgorithm = :brutetree\u001b[39m, since \u001b[36mbrutetree\u001b[39m\n",
       "       isn't actually a tree.\n",
       "\n",
       "    •  \u001b[36mreorder::Bool = true\u001b[39m : if \u001b[36mtrue\u001b[39m then points which are close in\n",
       "       distance are placed close in memory. In this case, a copy of the\n",
       "       original data will be made so that the original data is left\n",
       "       unmodified. Setting this to \u001b[36mtrue\u001b[39m can significantly improve\n",
       "       performance of the specified \u001b[36malgorithm\u001b[39m (except \u001b[36m:brutetree\u001b[39m). This\n",
       "       option is ignored and always taken as \u001b[36mfalse\u001b[39m for \u001b[36malgorithm =\n",
       "       :brutetree\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mweights::KNNKernel=Uniform()\u001b[39m : kernel used in assigning weights to\n",
       "       the k-nearest neighbors for each observation. An instance of one\n",
       "       of the types in \u001b[36mlist_kernels()\u001b[39m. User-defined weighting functions\n",
       "       can be passed by wrapping the function in a \u001b[36mUDF\u001b[39m kernel. If sample\n",
       "       weights \u001b[36mw\u001b[39m are passed during machine construction e.g\n",
       "       \u001b[36mmachine(model, X, y, w)\u001b[39m then the weight assigned to each neighbor\n",
       "       is the product of the \u001b[36mKNNKernel\u001b[39m generated weight and the\n",
       "       corresponding neighbor sample weight.\n",
       "\n",
       "  See also the package documentation\n",
       "  (https://github.com/KristofferC/NearestNeighbors.jl). For more information\n",
       "  about the kernels see the paper by Geler et.al Comparison of different\n",
       "  weighting schemes for the kNN classifier on time-series data\n",
       "  (https://perun.pmf.uns.ac.rs/radovanovic/publications/2016-kais-knn-weighting.pdf)."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@doc KNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d09aab6d-7969-46e6-b039-99f97eac6e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "DecisionTreeRegressor\n",
       "\\end{verbatim}\n",
       "A model type for constructing a CART decision tree regressor, based on \\href{https://github.com/bensadeghi/DecisionTree.jl}{DecisionTree.jl}, and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "\\begin{verbatim}\n",
       "DecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\n",
       "\\end{verbatim}\n",
       "Do \\texttt{model = DecisionTreeRegressor()} to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in \\texttt{DecisionTreeRegressor(max\\_depth=...)}.\n",
       "\n",
       "\\texttt{DecisionTreeRegressor} implements the \\href{https://en.wikipedia.org/wiki/Decision_tree_learning}{CART algorithm}, originally published in Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984): \"Classification and regression trees\". \\emph{Monterey, CA: Wadsworth \\& Brooks/Cole Advanced Books \\& Software.}.\n",
       "\n",
       "\\section{Training data}\n",
       "In MLJ or MLJBase, bind an instance \\texttt{model} to data with\n",
       "\n",
       "\\begin{verbatim}\n",
       "mach = machine(model, X, y)\n",
       "\\end{verbatim}\n",
       "where\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{X}: any table of input features (eg, a \\texttt{DataFrame}) whose columns each have one of the following element scitypes: \\texttt{Continuous}, \\texttt{Count}, or \\texttt{<:OrderedFactor}; check column scitypes with \\texttt{schema(X)}\n",
       "\n",
       "\n",
       "\\item \\texttt{y}: the target, which can be any \\texttt{AbstractVector} whose element scitype is \\texttt{Continuous}; check the scitype with \\texttt{scitype(y)}\n",
       "\n",
       "\\end{itemize}\n",
       "Train the machine with \\texttt{fit!(mach, rows=...)}.\n",
       "\n",
       "\\section{Hyper-parameters}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{max\\_depth=-1}:          max depth of the decision tree (-1=any)\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_samples\\_leaf=1}:    max number of samples each leaf needs to have\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_samples\\_split=2}:   min number of samples needed for a split\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_purity\\_increase=0}: min purity needed for a split\n",
       "\n",
       "\n",
       "\\item \\texttt{n\\_subfeatures=0}: number of features to select at random (0 for all, -1 for square root of number of features)\n",
       "\n",
       "\n",
       "\\item \\texttt{post\\_prune=false}:      set to \\texttt{true} for post-fit pruning\n",
       "\n",
       "\n",
       "\\item \\texttt{merge\\_purity\\_threshold=1.0}: (post-pruning) merge leaves having                          combined purity \\texttt{>= merge\\_purity\\_threshold}\n",
       "\n",
       "\n",
       "\\item \\texttt{rng=Random.GLOBAL\\_RNG}: random number generator or seed\n",
       "\n",
       "\\end{itemize}\n",
       "\\section{Operations}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{predict(mach, Xnew)}: return predictions of the target given new features \\texttt{Xnew} having the same scitype as \\texttt{X} above.\n",
       "\n",
       "\\end{itemize}\n",
       "\\section{Fitted parameters}\n",
       "The fields of \\texttt{fitted\\_params(mach)} are:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{tree}: the tree or stump object returned by the core DecisionTree.jl algorithm\n",
       "\n",
       "\\end{itemize}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "using MLJ\n",
       "Tree = @load DecisionTreeRegressor pkg=DecisionTree\n",
       "tree = Tree(max_depth=4, min_samples_split=3)\n",
       "\n",
       "X, y = make_regression(100, 2) # synthetic data\n",
       "mach = machine(tree, X, y) |> fit!\n",
       "\n",
       "Xnew, _ = make_regression(3, 2)\n",
       "yhat = predict(mach, Xnew) # new predictions\n",
       "\n",
       "fitted_params(mach).tree # raw tree or stump object from DecisionTree.jl\n",
       "\\end{verbatim}\n",
       "See also \\href{https://github.com/bensadeghi/DecisionTree.jl}{DecisionTree.jl} and the unwrapped model type \\href{@ref}{\\texttt{MLJDecisionTreeInterface.DecisionTree.DecisionTreeRegressor}}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "DecisionTreeRegressor\n",
       "```\n",
       "\n",
       "A model type for constructing a CART decision tree regressor, based on [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl), and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "```\n",
       "DecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\n",
       "```\n",
       "\n",
       "Do `model = DecisionTreeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DecisionTreeRegressor(max_depth=...)`.\n",
       "\n",
       "`DecisionTreeRegressor` implements the [CART algorithm](https://en.wikipedia.org/wiki/Decision_tree_learning), originally published in Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984): \"Classification and regression trees\". *Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software.*.\n",
       "\n",
       "# Training data\n",
       "\n",
       "In MLJ or MLJBase, bind an instance `model` to data with\n",
       "\n",
       "```\n",
       "mach = machine(model, X, y)\n",
       "```\n",
       "\n",
       "where\n",
       "\n",
       "  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n",
       "  * `y`: the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n",
       "\n",
       "Train the machine with `fit!(mach, rows=...)`.\n",
       "\n",
       "# Hyper-parameters\n",
       "\n",
       "  * `max_depth=-1`:          max depth of the decision tree (-1=any)\n",
       "  * `min_samples_leaf=1`:    max number of samples each leaf needs to have\n",
       "  * `min_samples_split=2`:   min number of samples needed for a split\n",
       "  * `min_purity_increase=0`: min purity needed for a split\n",
       "  * `n_subfeatures=0`: number of features to select at random (0 for all, -1 for square root of number of features)\n",
       "  * `post_prune=false`:      set to `true` for post-fit pruning\n",
       "  * `merge_purity_threshold=1.0`: (post-pruning) merge leaves having                          combined purity `>= merge_purity_threshold`\n",
       "  * `rng=Random.GLOBAL_RNG`: random number generator or seed\n",
       "\n",
       "# Operations\n",
       "\n",
       "  * `predict(mach, Xnew)`: return predictions of the target given new features `Xnew` having the same scitype as `X` above.\n",
       "\n",
       "# Fitted parameters\n",
       "\n",
       "The fields of `fitted_params(mach)` are:\n",
       "\n",
       "  * `tree`: the tree or stump object returned by the core DecisionTree.jl algorithm\n",
       "\n",
       "# Examples\n",
       "\n",
       "```\n",
       "using MLJ\n",
       "Tree = @load DecisionTreeRegressor pkg=DecisionTree\n",
       "tree = Tree(max_depth=4, min_samples_split=3)\n",
       "\n",
       "X, y = make_regression(100, 2) # synthetic data\n",
       "mach = machine(tree, X, y) |> fit!\n",
       "\n",
       "Xnew, _ = make_regression(3, 2)\n",
       "yhat = predict(mach, Xnew) # new predictions\n",
       "\n",
       "fitted_params(mach).tree # raw tree or stump object from DecisionTree.jl\n",
       "```\n",
       "\n",
       "See also [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl) and the unwrapped model type [`MLJDecisionTreeInterface.DecisionTree.DecisionTreeRegressor`](@ref).\n"
      ],
      "text/plain": [
       "\u001b[36m  DecisionTreeRegressor\u001b[39m\n",
       "\n",
       "  A model type for constructing a CART decision tree regressor, based on\n",
       "  DecisionTree.jl (https://github.com/bensadeghi/DecisionTree.jl), and\n",
       "  implementing the MLJ model interface.\n",
       "\n",
       "  From MLJ, the type can be imported using\n",
       "\n",
       "\u001b[36m  DecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree\u001b[39m\n",
       "\n",
       "  Do \u001b[36mmodel = DecisionTreeRegressor()\u001b[39m to construct an instance with default\n",
       "  hyper-parameters. Provide keyword arguments to override hyper-parameter\n",
       "  defaults, as in \u001b[36mDecisionTreeRegressor(max_depth=...)\u001b[39m.\n",
       "\n",
       "  \u001b[36mDecisionTreeRegressor\u001b[39m implements the CART algorithm\n",
       "  (https://en.wikipedia.org/wiki/Decision_tree_learning), originally published\n",
       "  in Breiman, Leo; Friedman, J. H.; Olshen, R. A.; Stone, C. J. (1984):\n",
       "  \"Classification and regression trees\". \u001b[4mMonterey, CA: Wadsworth & Brooks/Cole\n",
       "  Advanced Books & Software.\u001b[24m.\n",
       "\n",
       "\u001b[1m  Training data\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  In MLJ or MLJBase, bind an instance \u001b[36mmodel\u001b[39m to data with\n",
       "\n",
       "\u001b[36m  mach = machine(model, X, y)\u001b[39m\n",
       "\n",
       "  where\n",
       "\n",
       "    •  \u001b[36mX\u001b[39m: any table of input features (eg, a \u001b[36mDataFrame\u001b[39m) whose columns\n",
       "       each have one of the following element scitypes: \u001b[36mContinuous\u001b[39m,\n",
       "       \u001b[36mCount\u001b[39m, or \u001b[36m<:OrderedFactor\u001b[39m; check column scitypes with \u001b[36mschema(X)\u001b[39m\n",
       "\n",
       "    •  \u001b[36my\u001b[39m: the target, which can be any \u001b[36mAbstractVector\u001b[39m whose element\n",
       "       scitype is \u001b[36mContinuous\u001b[39m; check the scitype with \u001b[36mscitype(y)\u001b[39m\n",
       "\n",
       "  Train the machine with \u001b[36mfit!(mach, rows=...)\u001b[39m.\n",
       "\n",
       "\u001b[1m  Hyper-parameters\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mmax_depth=-1\u001b[39m: max depth of the decision tree (-1=any)\n",
       "\n",
       "    •  \u001b[36mmin_samples_leaf=1\u001b[39m: max number of samples each leaf needs to have\n",
       "\n",
       "    •  \u001b[36mmin_samples_split=2\u001b[39m: min number of samples needed for a split\n",
       "\n",
       "    •  \u001b[36mmin_purity_increase=0\u001b[39m: min purity needed for a split\n",
       "\n",
       "    •  \u001b[36mn_subfeatures=0\u001b[39m: number of features to select at random (0 for\n",
       "       all, -1 for square root of number of features)\n",
       "\n",
       "    •  \u001b[36mpost_prune=false\u001b[39m: set to \u001b[36mtrue\u001b[39m for post-fit pruning\n",
       "\n",
       "    •  \u001b[36mmerge_purity_threshold=1.0\u001b[39m: (post-pruning) merge leaves having\n",
       "       combined purity \u001b[36m>= merge_purity_threshold\u001b[39m\n",
       "\n",
       "    •  \u001b[36mrng=Random.GLOBAL_RNG\u001b[39m: random number generator or seed\n",
       "\n",
       "\u001b[1m  Operations\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mpredict(mach, Xnew)\u001b[39m: return predictions of the target given new\n",
       "       features \u001b[36mXnew\u001b[39m having the same scitype as \u001b[36mX\u001b[39m above.\n",
       "\n",
       "\u001b[1m  Fitted parameters\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  The fields of \u001b[36mfitted_params(mach)\u001b[39m are:\n",
       "\n",
       "    •  \u001b[36mtree\u001b[39m: the tree or stump object returned by the core\n",
       "       DecisionTree.jl algorithm\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  using MLJ\u001b[39m\n",
       "\u001b[36m  Tree = @load DecisionTreeRegressor pkg=DecisionTree\u001b[39m\n",
       "\u001b[36m  tree = Tree(max_depth=4, min_samples_split=3)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  X, y = make_regression(100, 2) # synthetic data\u001b[39m\n",
       "\u001b[36m  mach = machine(tree, X, y) |> fit!\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  Xnew, _ = make_regression(3, 2)\u001b[39m\n",
       "\u001b[36m  yhat = predict(mach, Xnew) # new predictions\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  fitted_params(mach).tree # raw tree or stump object from DecisionTree.jl\u001b[39m\n",
       "\n",
       "  See also DecisionTree.jl (https://github.com/bensadeghi/DecisionTree.jl) and\n",
       "  the unwrapped model type\n",
       "  \u001b[36mMLJDecisionTreeInterface.DecisionTree.DecisionTreeRegressor\u001b[39m."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc(\"DecisionTreeRegressor\", pkg=\"DecisionTree\")\n",
    "# DecisionTreeClassifier, DecisionTreeRegressor, RandomForestClassifier, RandomForestRegressor, AdaBoostStumpClassifier\n",
    "#?DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9650c1ce-2b1c-42bb-a22b-fe241caa142d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training Machine{DecisionTreeRegressor,…}.\n",
      "└ @ MLJBase C:\\Users\\sebas\\.julia\\packages\\MLJBase\\rMXo2\\src\\machines.jl:423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, measurement, operation, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_pairs\n",
       "Extract:\n",
       "┌───────────────────────────────────────────────────┬─────────────┬─────────────\n",
       "│\u001b[22m measure                                           \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m ⋯\n",
       "├───────────────────────────────────────────────────┼─────────────┼─────────────\n",
       "│ LPLoss(p = 1)                                     │ 0.286       │ predict    ⋯\n",
       "│ RootMeanSquaredError()                            │ 0.472       │ predict    ⋯\n",
       "│ RootMeanSquaredLogProportionalError(offset = 1.0) │ 0.328       │ predict    ⋯\n",
       "└───────────────────────────────────────────────────┴─────────────┴─────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tree = @load DecisionTreeRegressor pkg=DecisionTree verbosity=0\n",
    "\n",
    "ModelTree = Tree(max_depth=4, min_samples_split=3)\n",
    "\n",
    "mach_Tree = fit!(machine(ModelTree, X_train, Y_train))\n",
    "fitted_params(mach_Tree)\n",
    "Y_predict_Tree = round.(predict(mach_Tree, X_test))\n",
    "\n",
    "evaluate(ModelTree, X_train, Y_train,  measure=[l1, rms, rmslp1], verbosity=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a27e877d-d654-46ab-bcfd-e5ebdd462b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PerformanceEvaluation object with these fields:\n",
       "  measure, measurement, operation, per_fold,\n",
       "  per_observation, fitted_params_per_fold,\n",
       "  report_per_fold, train_test_pairs\n",
       "Extract:\n",
       "┌───────────────────────────────────────────────────┬─────────────┬─────────────\n",
       "│\u001b[22m measure                                           \u001b[0m│\u001b[22m measurement \u001b[0m│\u001b[22m operation \u001b[0m ⋯\n",
       "├───────────────────────────────────────────────────┼─────────────┼─────────────\n",
       "│ LPLoss(p = 1)                                     │ 0.594       │ predict    ⋯\n",
       "│ RootMeanSquaredError()                            │ 0.739       │ predict    ⋯\n",
       "│ RootMeanSquaredLogProportionalError(offset = 1.0) │ 0.51        │ predict    ⋯\n",
       "└───────────────────────────────────────────────────┴─────────────┴─────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ModelTree, X_test, Y_test,  measure=[l1, rms, rmslp1], verbosity=0) "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "1ade4eba-fc3b-430f-b7ea-05ac4ec2bccc",
   "lastKernelId": "47048d35-255a-478f-a93e-9210abbaabdd"
  },
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
